Multi-Modal LLM using Replicate LlaVa, Fuyu 8B, MiniGPT4 models for image reasoning#
In this notebook, we show how to use MultiModal LLM class for image understanding/reasoning. We now support:
In the 2nd part, we show how to use stream complete and aync complate for Replicate.
NOTE: At the moment, the Replicate multi-modal LLMs only support one image document at a time.
%pip install llama-index-multi-modal-llms-replicate
% pip install replicate
Load and initialize Replicate#
import os
REPLICATE_API_TOKEN = "" # Your Relicate API token here
os.environ["REPLICATE_API_TOKEN"] = REPLICATE_API_TOKEN
Download Images and Load Images locally#
from PIL import Image
import requests
from io import BytesIO
from llama_index.core.multi_modal_llms.generic_utils import load_image_urls
from llama_index.core.schema import ImageDocument
if not os.path.exists("test_images"):
os.makedirs("test_images")
# for now fuyu-8b model on replicate can mostly handle JPG image urls well instead of local files
image_urls = [
# "https://www.visualcapitalist.com/wp-content/uploads/2023/10/US_Mortgage_Rate_Surge-Sept-11-1.jpg",
"https://www.sportsnet.ca/wp-content/uploads/2023/11/CP1688996471-1040x572.jpg",
"https://res.cloudinary.com/hello-tickets/image/upload/c_limit,f_auto,q_auto,w_1920/v1640835927/o3pfl41q7m5bj8jardk0.jpg",
"https://www.cleverfiles.com/howto/wp-content/uploads/2018/03/minion.jpg",
]
# save images
for idx, image_url in enumerate(image_urls):
response = requests.get(image_url)
img = Image.open(BytesIO(response.content))
img.save(f"test_images/{idx}.png")
# option 1: load images from urls directly
# image_documents = load_image_urls(image_urls)
# option 2: load images from local
image_documents = [
ImageDocument(image_path=f"test_images/{idx}.png")
for idx in range(len(image_urls))
]
Visualize images#
import matplotlib.pyplot as plt
from llama_index.core.response.notebook_utils import display_image_uris
image_paths = [str(img_doc.image_path) for img_doc in image_documents]
display_image_uris(image_paths)
Provide various prompts to test different Multi Modal LLMs#
from llama_index.multi_modal_llms.replicate import ReplicateMultiModal
from llama_index.multi_modal_llms.replicate.base import (
REPLICATE_MULTI_MODAL_LLM_MODELS,
)
prompts = [
"what is shown in this image?",
"how many people are shown in the image?",
"is there anything unusual in the image?",
]
Generate Image Reasoning from different LLMs with different prompts for different images#
res = []
for prompt_idx, prompt in enumerate(prompts):
for image_idx, image_doc in enumerate(image_documents):
for llm_idx, llm_model in enumerate(REPLICATE_MULTI_MODAL_LLM_MODELS):
try:
## Initialize the MultiModal LLM model
multi_modal_llm = ReplicateMultiModal(
model=REPLICATE_MULTI_MODAL_LLM_MODELS[llm_model],
max_new_tokens=100,
temperature=0.1,
num_input_files=1,
top_p=0.9,
num_beams=1,
repetition_penalty=1,
)
mm_resp = multi_modal_llm.complete(
prompt=prompt,
image_documents=[image_doc],
)
except Exception as e:
print(
f"Error with LLM model inference with prompt {prompt}, image {image_idx}, and MM model {llm_model}"
)
print("Inference Failed due to: ", e)
continue
res.append(
{
"model": llm_model,
"prompt": prompt,
"response": mm_resp,
"image": str(image_doc.image_path),
}
)
Display Sampled Responses from Multi-Modal LLMs#
from IPython.display import display
import pandas as pd
pd.options.display.max_colwidth = None
df = pd.DataFrame(res)
display(df[:5])
Human Label the Correctness and Relevance of the Multi-Modal LLM Reasoning Results#
Note that Human Lable could have some bias/subjectivity when label relevance and correctness.
We label the Correctness and Relevance scores between [1, 5]
5: perfectly answer the question
4: somehow answer the question
3: partly answer the question
2: answer the question with wrong answer
1: no answer or
hallucination
Summary of preliminary findings with evaluated Multi-Modal Models#
First, the purpose of this notework is to show how to leverage Replicate for serving different Multi-Modal LLMs for image reasoning tasks. There are some limitations with such comparison:
We compared and evaluated LLaVa-13B, Fuyu-8B, and MiniGPT-4 for some simple and limited tasks/prompts.
Note that
the hyperparameters for different models are the same in the example. The power of hyperparamters tuning could be significant for the quality MM LLMs models.
Human evaluation could have some Bias/Subjectivity/Noise
Some preliminary findings:
MiniGPT-4sometimes can yield a more accurate answer like
There are two minions in the image.instead of
There are two people shown in the image.from
LlaVaor
Fuyu-8B. Another example is that
MiniGPT-4answers
Colosseumdirectly for the question
what is it in the imagefor the Italy Colosseum image.
MiniGPT-4failed to give results for two prompts. It answers
I'm sorry, but I cannot see the image you provided.But it can answer other questions for the same images. Not sure it is an issue of Replicate inference or MiniGPT-4 model itself
Fuyu-8Band
LlaVa-13Busually yield longer verbose answers to the question with more context to support.
Llava-13Band
Fuyu-8Bsometimes yield slightly higher
hallucinationespeically for the question
is there anything unusual in the image?
Replicate Stream Complete, Async Complete, Async Stream Complete Mode#
Init Fuyu-8B Model#
multi_modal_llm = ReplicateMultiModal(
model=REPLICATE_MULTI_MODAL_LLM_MODELS["fuyu-8b"],
max_new_tokens=100,
temperature=0.1,
num_input_files=1,
top_p=0.9,
num_beams=1,
repetition_penalty=1,
)
Using async stream complete#
resp = await multi_modal_llm.astream_complete(
prompt="tell me about this image",
image_documents=[image_documents[0]],
)
async for delta in resp:
print(delta.delta, end="")
The image features a man wearing a suit and tie, standing in front of a stage with a backdrop. He is holding a golden ball trophy, possibly an award, in his hands. The man appears to be posing for a photo, possibly celebrating his achievement or receiving an award.
In the background, there are multiple people visible, possibly attending or participating in the event. The backdrop appears to be a large screen, possibly displaying information about the event or ceremony.
Using async complete#
resp = await multi_modal_llm.acomplete(
prompt="tell me about this image",
image_documents=[image_documents[0]],
)
print(resp)
The image features a man wearing a suit and tie, standing in front of a stage with a backdrop. He is holding a golden ball trophy, possibly an award, in his hands. The man appears to be posing for a photo, possibly celebrating his achievement or receiving an award.
In the background, there are multiple people visible, possibly attending or participating in the event. The backdrop appears to be a large screen, possibly displaying information about the event or ceremony.
Using stream complete#
resp = multi_modal_llm.stream_complete(
prompt="tell me about this image",
image_documents=[image_documents[0]],
)
for delta in resp:
print(delta.delta, end="")
The image features a man wearing a suit and tie, standing in front of a stage with a backdrop. He is holding a golden ball trophy, possibly an award, in his hands. The man appears to be posing for a photo, possibly celebrating his achievement or receiving an award.
In the background, there are multiple people visible, possibly attending or participating in the event. The backdrop appears to be a large screen, possibly displaying information about the event or ceremony.